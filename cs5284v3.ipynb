{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_1nwalsd_vSJ","outputId":"746df4fd-df77-4705-ff2e-344d7adf6214","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_addr = '/content/drive/MyDrive/cs5284pro/dow_1day_price.csv'\n","adj_addr = '/content/drive/MyDrive/cs5284pro/dow_1day_090_01_corr.csv'\n","s_index = 0\n","lr = 1e-3\n","n_neurons = 128\n","seq_len = 12\n","n_epochs = 40\n","batch_size = 128\n","n_off = 0\n","th = 0.2"],"metadata":{"id":"WDdjYAs5_oTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import scipy\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error\n","from math import sqrt\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","import scipy.sparse as sp\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","class GraphTransformerLayer(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_heads, dropout=0.1):\n","        super(GraphTransformerLayer, self).__init__()\n","        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim * 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim * 2, hidden_dim),\n","        )\n","        self.norm1 = nn.LayerNorm(hidden_dim)\n","        self.norm2 = nn.LayerNorm(hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: [batch_size, seq_len, input_dim]\n","        \"\"\"\n","        if x.dim() == 2:\n","            x = x.unsqueeze(-1)\n","\n","        # Transpose for MultiheadAttention\n","        x = x.permute(1, 0, 2)  # [seq_len, batch_size, input_dim]\n","\n","        # Self-attention\n","        attn_output, _ = self.attention(x, x, x)\n","        attn_output = attn_output.permute(1, 0, 2)  # [batch_size, seq_len, hidden_dim]\n","\n","        # Residual connection and feedforward network\n","        x = self.norm1(x.permute(1, 0, 2) + attn_output)\n","        ffn_output = self.ffn(x)\n","        x = self.norm2(x + self.dropout(ffn_output))\n","\n","        return x\n","\n","class TfGRUCell(nn.Module):\n","    def __init__(self, num_units, adj, num_gcn_nodes, s_index, hidden_dim=1, num_heads=1, dropout=0.1):\n","        super(TfGRUCell, self).__init__()\n","        self.units = num_units\n","        self._gcn_nodes = num_gcn_nodes\n","        self.s_index = s_index\n","\n","        # Preprocess adjacency matrix\n","        adj = self.calculate_laplacian(adj)\n","        # if isinstance(adj, sp.coo_matrix):\n","        # adj = adj.toarray()  # Convert to a dense NumPy array\n","        self.register_buffer(\"_adj\", adj)  # Save as a non-trainable buffer\n","\n","        # Graph Transformer layer\n","        self.graph_transformer = GraphTransformerLayer(\n","            input_dim=self.units,\n","            hidden_dim=hidden_dim,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","        )\n","\n","        # GRU weights\n","        self.wz = nn.Linear(hidden_dim, self.units, bias=True)\n","        self.wr = nn.Linear(hidden_dim, self.units, bias=True)\n","        self.wh = nn.Linear(hidden_dim, self.units, bias=True)\n","\n","        self.uz = nn.Linear(self.units, self.units, bias=False)\n","        self.ur = nn.Linear(self.units, self.units, bias=False)\n","        self.uh = nn.Linear(self.units, self.units, bias=False)\n","\n","    @property\n","    def state_size(self):\n","        return self.units\n","\n","    def calculate_laplacian(self, adj):\n","        adj = adj + torch.eye(adj.size(0))  # Add self-loops\n","        degree = torch.sum(adj, dim=1)\n","        degree[degree==0] = 1\n","        d_inv_sqrt = torch.diag(torch.pow(degree, -0.5))\n","        laplacian = d_inv_sqrt @ adj @ d_inv_sqrt\n","        return laplacian\n","\n","    def trans(self, inputs):\n","        # print(inputs.shape)\n","        if inputs.dim() == 2:\n","            inputs = inputs.unsqueeze(-1)  # Add feature dimension\n","\n","        # Use Graph Transformer for feature extraction\n","        transformed_inputs = self.graph_transformer(inputs)\n","        return transformed_inputs\n","\n","    def forward(self, inputs, state):\n","        x = self.trans(inputs)\n","\n","        # GRU gates\n","        z = torch.sigmoid(self.wz(x.mean(dim=1)) + self.uz(state))\n","        r = torch.sigmoid(self.wr(x.mean(dim=1)) + self.ur(state))\n","        h = torch.tanh(self.wh(x.mean(dim=1)) + self.uh(r * state))\n","\n","        # Update state\n","        output = z * state + (1 - z) * h\n","        return output\n","\n"],"metadata":{"id":"XUd7fJVV_iqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TfGRUModel(nn.Module):\n","    def __init__(self, cell, seq_len, num_gcn_nodes):\n","        super(TfGRUModel, self).__init__()\n","        self.cell = cell\n","        self.seq_len = seq_len\n","        self.num_gcn_nodes = num_gcn_nodes\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        state = torch.zeros(batch_size, self.cell.units).to(x.device)\n","\n","        outputs = []\n","        for t in range(self.seq_len):\n","            state = self.cell(x[:, t, :], state)\n","            outputs.append(state)\n","\n","        return torch.stack(outputs, dim=1)  # (batch_size, seq_len, units)\n"],"metadata":{"id":"BMKeLSpD_aBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_adj(adj):\n","    \"\"\"Normalize the adjacency matrix A_hat = D^-0.5 A D^-0.5\"\"\"\n","    adj = sp.coo_matrix(adj)\n","    rowsum = np.array(adj.sum(1))\n","    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n","    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n","    normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n","    return normalized_adj.astype(np.float32)\n","\n","def load_dow_price_data(data_addr, adj_addr):\n","    data = pd.read_csv(data_addr).values\n","    adj = pd.read_csv(adj_addr, header=None).values\n","    # data = normalize(data, axis=0)\n","    scaler = MinMaxScaler()\n","    data = scaler.fit_transform(data)\n","    return data, adj\n","\n","def preprocess_data(data, labels, time_len, train_rate, seq_len, pre_len):\n","    X, Y, pre_Y = [], [], []\n","    for i in range(time_len - seq_len - pre_len):\n","        X.append(data[i:i + seq_len, :])\n","        Y.append(labels[i + seq_len:i + seq_len + pre_len])\n","        pre_Y.append(labels[(i + seq_len - 1):(i + seq_len + pre_len - 1)])\n","\n","    # Split the dataset into training and testing sets\n","    train_size = int(train_rate * len(X))\n","    X_train = np.array(X[:train_size])\n","    Y_train = np.array(Y[:train_size])\n","    X_test = np.array(X[train_size:])\n","    Y_test = np.array(Y[train_size:])\n","    # pre_Y_test = labels[train_size + seq_len:train_size + seq_len + len(X_test)]\n","    pre_Y_test = np.array(pre_Y[train_size:])\n","\n","    return X_train, Y_train, X_test, Y_test, pre_Y_test"],"metadata":{"id":"yUbk0VysIX0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load and preprocess data\n","data, adj = load_dow_price_data(data_addr, adj_addr)\n","adj = normalize_adj(adj)\n","if isinstance(adj, sp.coo_matrix):\n","    adj = adj.toarray()  # Convert to a dense NumPy array\n","labels = data[:, s_index]\n","if n_off > 0:\n","    data = data[:-n_off]\n","    labels = labels[n_off:]\n","\n","train_rate = 0.8\n","pre_len = 1\n","time_len = data.shape[0]\n","n_gcn_nodes = data.shape[1]\n","\n","X_train, y_train, X_test, y_test, pre_y_test = preprocess_data( data, labels, time_len, train_rate, seq_len, pre_len)\n","\n","# X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","# X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","X_train = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)  # [batch_size, num_gcn_nodes, seq_len]\n","X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n"],"metadata":{"id":"qSwSqbdBAad7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9152fd34-62f2-4b63-a5bd-da381ce0983a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-049782d7aeb2>:5: RuntimeWarning: divide by zero encountered in power\n","  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"]}]},{"cell_type":"code","source":["# Initialize model\n","cell = TfGRUCell(n_neurons, torch.tensor(adj, dtype=torch.float32), n_gcn_nodes, s_index)\n","model = TfGRUModel(cell, seq_len, n_gcn_nodes)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.MSELoss()\n","# Training\n","for epoch in range(n_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train).squeeze()\n","    loss = criterion(outputs[:, -1, 0].unsqueeze(1), y_train) # Select the first output node for comparison with y_train\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngBdnOgqKV5I","outputId":"8a78fe29-dd66-40bd-ed45-45042656bedc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40, Loss: 0.08105330914258957\n","Epoch 2/40, Loss: 0.043260443955659866\n","Epoch 3/40, Loss: 0.030466580763459206\n","Epoch 4/40, Loss: 0.033168692141771317\n","Epoch 5/40, Loss: 0.03996584191918373\n","Epoch 6/40, Loss: 0.04384530335664749\n","Epoch 7/40, Loss: 0.0436837337911129\n","Epoch 8/40, Loss: 0.040774375200271606\n","Epoch 9/40, Loss: 0.03679312393069267\n","Epoch 10/40, Loss: 0.03316093981266022\n","Epoch 11/40, Loss: 0.030823804438114166\n","Epoch 12/40, Loss: 0.030137624591588974\n","Epoch 13/40, Loss: 0.03085392713546753\n","Epoch 14/40, Loss: 0.032274309545755386\n","Epoch 15/40, Loss: 0.03357940539717674\n","Epoch 16/40, Loss: 0.03418903425335884\n","Epoch 17/40, Loss: 0.033944204449653625\n","Epoch 18/40, Loss: 0.033054500818252563\n","Epoch 19/40, Loss: 0.03191396966576576\n","Epoch 20/40, Loss: 0.030909720808267593\n","Epoch 21/40, Loss: 0.03029399923980236\n","Epoch 22/40, Loss: 0.030137933790683746\n","Epoch 23/40, Loss: 0.030354419723153114\n","Epoch 24/40, Loss: 0.030763156712055206\n","Epoch 25/40, Loss: 0.031167425215244293\n","Epoch 26/40, Loss: 0.03141692653298378\n","Epoch 27/40, Loss: 0.03144174441695213\n","Epoch 28/40, Loss: 0.031255386769771576\n","Epoch 29/40, Loss: 0.03093411959707737\n","Epoch 30/40, Loss: 0.03058331459760666\n","Epoch 31/40, Loss: 0.030301734805107117\n","Epoch 32/40, Loss: 0.030152738094329834\n","Epoch 33/40, Loss: 0.03014928288757801\n","Epoch 34/40, Loss: 0.030256196856498718\n","Epoch 35/40, Loss: 0.030407940968871117\n","Epoch 36/40, Loss: 0.030534951016306877\n","Epoch 37/40, Loss: 0.030588332563638687\n","Epoch 38/40, Loss: 0.030553890392184258\n","Epoch 39/40, Loss: 0.030451491475105286\n","Epoch 40/40, Loss: 0.03032219409942627\n"]}]},{"cell_type":"code","source":["# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(X_test)\n","\n","    result = predictions[:, -1, 0].unsqueeze(1).numpy()\n","\n","# Metrics\n","# print(result.shape)\n","# print(y_test.shape)\n","# r2 = r2_score(y_test, result)\n","rmse = sqrt(mean_squared_error(y_test, result))\n","mae = mean_absolute_error(y_test, result)\n","# re = avg_relative_error(y_test, result)\n","\n","print(\"***********************\")\n","# print(f\"R2: {r2}\")\n","print(f\"RMSE: {rmse}\")\n","print(f\"MAE: {mae}\")\n","# print(f\"Relative Error: {re}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNSZrQ1rLHo9","outputId":"82f87513-64af-45d7-ef7b-51d035095afb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["***********************\n","RMSE: 0.020449883523973086\n","MAE: 0.01664881408214569\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}